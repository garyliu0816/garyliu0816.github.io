{"pages":[{"title":"","text":"efUSL9Wuh9","link":"/baidu_verify_efUSL9Wuh9.html"}],"posts":[{"title":"Anaconda常用指令","text":"1. 查看包conda list # 查看安装了哪些包conda env list # 查看有哪些虚拟环境conda -V # 查看conda的版本 2. 创建虚拟环境conda create -n [虚拟环境名] python=[python版本] [要安装的包...] conda create -n python3 python=3.6 numpy # 创建一个名为python3，python版本为3.6，安装了numpy的环境conda create -n backup --clone root # 创建一个名为backup的与root一样的环境 3. 切换环境source activate [环境名] # Linuxactivate [环境名] # Windows 4. 关闭环境source deactivate # Linuxdeactivate # Windows 5. 指定虚拟环境安装包conda install -n [虚拟环境名] [包名]# 或者可以先进入对应虚拟环境在执行pippip install [包名] 6. 移除虚拟环境conda remove --name [虚拟环境名] [包名] # 移除某个环境中的包conda remove -n [虚拟环境名] --all # 移除某个虚拟环境 7. 将环境备份至requirement.txtpip freeze &gt; requirements.txt 8. 安装备份的环境pip install -r requirements.txt","link":"/Anaconda常用指令.html"},{"title":"Good Semi-supervised learning That requires a Bad GAN论文阅读笔记","text":"前置知识GAN（生成对抗网络）在semi-supervised learning（半监督学习）上取得了较强的实证成果，但是有两点是我们都没搞明白的： discriminator（判别器）是如何从与generator（生成器）的联合训练中收益的 为什么一个好的classification（分类）效果和一个好的生成器不能同时获得 本质目标为了更好的寻找能正确分类的low-density boundary（低密度边界） ![low-density boundary](https://garyliu0816.github.io/images/bad_gan_1.png) 为了防止非正确分类低密度边界的干扰，我们需要用生成器产生可见样本空间中的“补样”，填充一些低密度区域，此时的生成器产生的样本分布与真实分布是不一致的，因此生成器是“bad”的。 OpenAI提出的基于GAN的半监督学习的优化目标（损失函数）如下： ![semi-supervised-gan](https://garyliu0816.github.io/images/bad_gan_2.png) ### 参数说明 - 标注集L={(x,y)}，其中x为图像数据，{1,2,…,K}对应K个真类，还有一个K+1的假类 - P_G、P_D是对应于G（生成器）和D（判别器）的概率分布，P_D覆盖K+1个类的分布，前K个类是真类，第K+1个类是假类（生成器生成的），和CatGAN一样 - P是真实数据的概率分布 这个损失函数由三部分组成。第一部分是最大化在标注集L中log下的P_D条件概率的数学期望，这个是基本的监督学习的损失函数；第二部分是最大化在真实数据p中log下的P_D对前K个类的概率的数学期望；第三部分是最大化在生成数据P_G中log下P_D对第K+1类概率的数学期望。 这样看还是太复杂了吧，我这里用一些简单的语言来解释： 第一部分，是希望D能从标注数据L中接收任意输入（x，y），得到的输出y^与输入时的y一致，也就是正确分类，然后将这种概率最大化。 第二部分，是希望D能从真实数据P（包含未标注数据）中接收任意输入x，得到的输出y被认为是真类，将这种概率最大化。 第三部分，是希望D能从生成数据P_G（都是假的）中接收任意输入x，得到的输出y被认为是假类，将这种概率最大化。这里的D实际上就是一个多分类神经网络，D的最后一层就是一个softmax激活函数。 理论推导![proposition1](https://garyliu0816.github.io/images/bad_gan_3.png) ### 提议1 如果生成器D是完美的，即P_D与P的分布完全一致，那么公式的第二部分和第三部分就是矛盾的，如果第二部分大，则第三部分必然很小，反之亦然。 因此，当生成器是完美的时候，不仅生成器不起作用，而且未标记的数据也没有得到有效利用。 ![assumption1](https://garyliu0816.github.io/images/bad_gan_4.png) ### 假设1 收敛条件。当D收敛于一个训练集{L,U,G}（L是标注数据，U是未标注数据，G是生成数据），D能学到一个完全能正确分类的决策边界。更具体来说，（1）L中的数据均能正确分类；（2）G中的数据均被分在第K+1类（3）U中的数据均被分在前K个类中。 ![lemma1](https://garyliu0816.github.io/images/bad_gan_5.png) ![corollary1](https://garyliu0816.github.io/images/bad_gan_6.png) ### 推论1 当生成的数据是无穷多的时候，根据Lemma1，我们有以下推论，D在softmax前输出的属于前K个类的值均小于等于0 ![proposition2](https://garyliu0816.github.io/images/bad_gan_7.png) ### 提议2 在满足推论1的情况下，我们所有的分类集合均正确分类。 直观地说，生成器生成补码样本，因此真实类的logits在补码样本中必须很低。因此，该鉴别器在低密度区域获得了类边界。 ![pic1](https://garyliu0816.github.io/images/bad_gan_8.png \"黑色为真实数据，白色为稀疏区域（此图为上帝视角的Ground Truth）\") ![pic2](https://garyliu0816.github.io/images/bad_gan_9.png \"黑色为生成数据，白色为稀疏区域\") 实验方法Generator Entropy（生成熵）![Generator Entropy](https://garyliu0816.github.io/images/bad_gan_10.png) 这里可以用L_PT代替 ![L_PT](https://garyliu0816.github.io/images/bad_gan_11.png) ### Generating Low-Density Samples（生成低密度样本） 最小化它 ![Generating Low-Density Samples](https://garyliu0816.github.io/images/bad_gan_12.png) ### 生成器的目标函数（损失函数） 也就是生成熵+生成低密度样本 ![loss1](https://garyliu0816.github.io/images/bad_gan_13.png) ### 判别器的目标函数（损失函数） ![loss2](https://garyliu0816.github.io/images/bad_gan_14.png) 实验结果当然是SOTA啦！ ![result](https://garyliu0816.github.io/images/bad_gan_15.png)","link":"/Good Semi-supervised learning That requires a Bad GAN论文阅读笔记.html"},{"title":"Re：从零开始的行人重识别（三）","text":"度量学习之对比损失 Hadsell R, Chopra S, LeCun Y. Dimensionality reduction by learning an invariant mapping[C]//2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06). IEEE, 2006, 2: 1735-1742. 这篇论文早在2006年就被提出来，其中最重要的核心思想就是减小类内距离，扩大类间距离。因此作者提出一个对比损失（contrastive loss）函数：$$L(W,Y,\\vec{X_1},\\vec{X_2})=(1-Y)\\frac{1}{2}{(D_W)}^2+(Y)\\frac{1}{2}{(\\max{0,m-D_W})}^2$$其中，$$D_{W}(\\vec{X_1},\\vec{X_2})={\\left|G_W(\\vec{X_1})-G_W(\\vec{X_2})\\right|}_2$$ 这个损失函数其实可以当做两部分来看，第一部分是当$\\vec{X_1}$与$\\vec{X_2}$的分类或者标签又或者是身份相同时$Y=0$，此时的函数就变成了，$$L_S(W,\\vec{X_1},\\vec{X_2})=\\frac{1}{2}{(D_W)}^2$$此时$\\vec{X_1}$与$\\vec{X_2}$的距离越小损失函数越小，否则当$\\vec{X_1}$与$\\vec{X_2}$的分类不同时$Y=1$，那么此时的函数可以看做成，$$L_D(W,\\vec{X_1},\\vec{X_2})=\\frac{1}{2}{(max{ 0,m-D_W })}^2$$此时$\\vec{X_1}$与$\\vec{X_2}$的距离越大损失函数越小。下图可以很好地表示Loss值与$D_W$的关系，其中蓝色的线表示$Y=1$时的损失变化曲线，红色的线表示$Y=0$时的损失变化曲线： ![Graph of the loss function L against the energy DW](https://garyliu0816.github.io/images/ReID_2.png) 通过这种方式，对模型进行训练，就会使得模型中同一类的特征相互吸引，不同类的特征相互排斥，最后达到不同类之间的距离均大于同类之间的距离。以下是该算法应用在MNIST手写数字识别数据集上的可视化效果图： ![placement of the test samples in output space](https://garyliu0816.github.io/images/ReID_3.png)","link":"/Re：从零开始的行人重识别（三）.html"},{"title":"Re：从零开始的行人重识别（一）","text":"绪言马上研二要开题了，提前通过blog的方式先把研究思路理清楚，也算是重新温习一下学习过程中遇到的问题吧，希望能给自己带来一些新的思路，同时帮助一些刚刚接触reID的同学入门（虽然应该没什么人会看），如有勘误可以提出来，我会及时改正的。 什么是行人重识别？行人重识别的是属于计算机视觉下的重要的也是当前比较困难的任务。它主要要解决问题是如何在不同的摄像头下找到相同的一个人，这种问题其实可以看做是一个图像检索的问题，即是通过一张行人在一个摄像头下的图像（或视频）作为检索目标，在其余摄像头中找到与之最相似的行人。 为什么需要行人重识别？主要是由于当前用于行人检测的摄像头大多是固定位置的局限性，当行人离开摄像头后，就无法继续追踪新摄像头下的行人，当行人重识别与行人检测技术相结合后，就可以弥补缺陷。行人重识别广泛应用于智能视频监控、智能安保等领域。 行人重识别问题的难点 在监控摄像头下的人脸往往模糊无法看清，因此不能直接使用人脸的信息作为识别的特征。 不同的摄像头下因光照、行人的尺寸、行人的姿态，行人的朝向的不同，导致行人不能只靠颜色、轮廓等信息进行重识别。 当前的行人重识别模型的泛化能力还有所欠缺，在一个数据集上训练的数据无法在另一个数据集上也有较好的表现。 行人重识别常用数据集介绍Market1501 Zheng L, Shen L, Tian L, et al. Scalable person re-identification: A benchmark[C]//Proceedings of the IEEE international conference on computer vision. 2015: 1116-1124. 2015年清华大学整理并提出，采集自6个摄像头（其中5个高清摄像头和1个低清摄像头）。Market1501包含了12,936张训练（train）图像，3,368张查询（query）图像，以及19,732张图库（gallery）图像，数据集包含1501个ID，其中训练集包含751个ID的图像，图库集包含750个ID的图像。 下载链接 DukeMTMC-reID Ristani E, Solera F, Zou R, et al. Performance measures and a data set for multi-target, multi-camera tracking[C]//European Conference on Computer Vision. Springer, Cham, 2016: 17-35. 2017年Duke大学提出，采集自8个不同的摄像头，是DukeMTMC数据集的子集，DukeMTMC-reID包含了16,522张训练图像，2,228个查询图像，以及17,661张图库图像，数据集包含1404个ID，其中训练集包含702个ID的图像，图库集包含702个ID的图像。 下载链接提取码：bhbh CUHK03 Li W, Zhao R, Xiao T, et al. Deepreid: Deep filter pairing neural network for person re-identification[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2014: 152-159. 2014年香港中文大学大学提出，采集自10个不同的摄像头，CUHK03包含了13164 张图像，数据集包含1360个ID，其中有1160个ID属于训练集，100个ID属于验证集，100个ID属于测试集。 下载链接提取码：rhjq 评价指标Rank-n搜索结果中top-n张图有正确结果的概率。 例如： query image为 m1 ，在大小为100的gallery中搜索。 如果识别结果是 m1 、m2、m3、m4、m5……，则此时rank-1的正确率为100%，rank-2的正确率也为100%，rank-5的正确率也为100%；如果识别结果是m2、 m1 、m3、m4、m5……，则此时rank-1的正确率为0%，rank-2的正确率为100%，rank-5的正确率也为100%；如果识别结果是m2、m3、m4、m5、 m1 ……，则此时rank-1的正确率为0%，rank-2的正确率为0%，rank-5的正确率为100%最后求得多个query的Rank-n取平均值。 CMC全称为Cumulative Match Characteristic，将一个摄像头采集的图像用作查询集，将其余摄影机采集的图像用作图库集。对于图库集，为每个ID随机采样一张图像。对于查询集，将使用所有图像，获取每个图像的CMC曲线，这里的CMC曲线就是Rank-n与Accuracy的关系曲线，然后对它们进行平均。重复此评估过程100次，并将平均值提交为最终结果。 mAP能够反应query image在gallery中所有正确的图片排在检索列表前面的程度，能更加全面的衡量ReID算法的性能。假设query image在gallery中有4张待查询图像，在检索的列表中排序分别为1、2、5、7，则ap为（1/1 + 2/2 + 3/5 + 4/7）/4=0.793。ap较大时，检索结果都相对靠前，最后对所有query的ap取平均值得到mAP。","link":"/Re：从零开始的行人重识别（一）.html"},{"title":"Markdown常用标记","text":"1. 标题# 一级标题## 二级标题 一级标题二级标题2. 分割线--- 3. 引用&gt;引用&gt;&gt;嵌套引用 引用 嵌套引用 4. 字体*倾斜***粗体*****倾斜加粗***~~删除线~~ 倾斜粗体倾斜加粗删除线 5. 超链接[超链接名](超链接地址 &quot;超链接title&quot;)![图片名](图片地址 &quot;图片title&quot;) 必应 6. 代码&apos;代码&apos;&apos;&apos;&apos;代码块&apos;&apos;&apos; print(&quot;hello world!&quot;) print(&quot;hello world!&quot;)print(&quot;hello world!!!&quot;) 7. 列表- [x] 支持以 PDF 格式导出文稿- [x] 改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率- [ ] 新增 Todo 列表功能- [ ] 修复 LaTex 公式渲染问题- [ ] 新增 LaTex 公式编号功能 支持以 PDF 格式导出文稿 改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率 新增 Todo 列表功能 修复 LaTex 公式渲染问题 新增 LaTex 公式编号功能 8. 表格| 项目 | 价格 | 数量 || - | :-: | -: || 计算机 | $1600 | 5 || 手机 | $12 | 12 || 管线 | $1 | 234 | 项目 价格 数量 计算机 $1600 5 手机 $12 12 管线 $1 234","link":"/Markdown常用标记.html"},{"title":"tensorflow+cuda+cudnn安装成功的版本（持续更新）","text":"以下版本经过本人亲自测试可以安装使用 tensorflow cuda python cudnn visual C++ redistributable build tools gpu-1.13.1 10.1 3.7 7 2015 update 3 2015 Update 3 gpu-1.13.1 10.0 3.6 7 2015 update 3 2015 Update 3 gpu-1.12.0 9.0 3.5 7 2015 update 3 2015 Update 3","link":"/tensorflow-cuda-cudnn安装成功的版本（持续更新）.html"},{"title":"Re：从零开始的行人重识别（四）","text":"度量学习之三元组损失 Schroff F, Kalenichenko D, Philbin J. Facenet: A unified embedding for face recognition and clustering[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 815-823. 这篇论文虽然是是用在人脸识别上的，但是行人重识别中的一个影响力十分大损失函数——triplet loss就是在这篇论文中提出的，之前用到的对比损失（contrastive loss）能够起到缩小类内距离，增大类间距离的作用，从而通过距离判断两类相似与否。但是contrastive loss有一个痛点，就是我们无法约束类间距离和类内距离的大小，虽然缩小了类内距离，也扩大了类间距离，但是类内距离仍然可能比类间距离大，这就是contrastive loss的局限性。由此作者想到增加一个类内距离比类间距离小的约束，于是三元组诞生了，如下图所示： ![Triplet Loss](https://garyliu0816.github.io/images/ReID_4.png) 这里的Anchor指的就是我们的查询图像，Positive是与Anchor一类的图像，Negative是与Anchor不同类的图像，将这三种图像通过CNN提取得到的特征计算欧式距离，放入损失函数中进行计算，就得到了我们的triplet loss： $$L = \\sum_\\limits{i}^{N}{[{\\left| f(x_{i}^{a})-f(x_{i}^{p}) \\right |}{2}^{2}-{\\left| f(x{i}^{a})-f(x_{i}^{n}) \\right |}{2}^{2} + \\alpha]}{+}$$ 值得注意这里的loss设置了一个超参数$\\alpha$，这个参数的作用是要让类内距离不止比类间距离小，还要小一个$\\alpha$的距离。 这个损失函数的基本思想是比较浅显易懂的，但是要让这个损失函数运作起来才是重点。在预训练好的数据集上，由于大多数的类内图像的距离就已经比类间图像的距离小了，因此损失值计算得到的就是0，则模型参数就不会发生更新，如果采用暴力的方式穷举所有可能，那么会浪费大量的算力在无用的计算上，这里的计算复杂度大致为$O(n^5)$，所以是无法穷举的，那么这里作者提出一个Generate triplets Online的方法。也就是在每一批数据中找到一个Positive使之与Anchor之间的距离最大同时再找到一个Negative使之与Anchor之间的距离最小，这样的三元组往往是有效的。 最后作者还讨论了一下各种因素对模型的影响： 图像质量： ![Image Quality](https://garyliu0816.github.io/images/ReID_5.png) 嵌入维度： ![Embedding Dimensionality](https://garyliu0816.github.io/images/ReID_6.png) 数据集大小： ![Training Data Size](https://garyliu0816.github.io/images/ReID_7.png) 可以从上图得知嵌入维度在128时效果最好，除此之外，图像质量（尺寸）和数据集大小都是越大好。","link":"/Re：从零开始的行人重识别（四）.html"},{"title":"Re：从零开始的行人重识别（二）","text":"用分类来学习行人特征 Geng M, Wang Y, Xiang T, et al. Deep transfer learning for person re-identification[J]. arXiv preprint arXiv:1611.05244, 2016. 该论文的提出的模型是使用的ImageNet上训练好的模型进行初始化，然后对特征采用了两个子网络进行训练，一个是分类子网络，一个是验证子网络。分类子网络的输入是一张一张的图像，采用的损失函数是cross-entropy loss，这样使得类间特征的距离扩大。验证子网络的输入是成对的图像，值得注意的是这里采用的损失函数不是contrastive loss而是与之前一样的cross-entropy loss，参考作者给出的原因是他们发现用contrastive loss反而模型性能会变差，具体操作是将成对的图像特征相减，然后当做是二分类模型，输出相似或者不相似，这样能使类内特征的距离减小。 该模型的结构如下： ![deep Re-ID network architecture](https://garyliu0816.github.io/images/ReID_1.png) 论文中的细节： base network采用的是在ImageNet上预训练过的GoogLeNet，发现效果和ResNet相当。 提出一种Loss specific dropout unit，该单元特殊用于验证子网络，因为要对两个图像进行相减操作，因此两张图像的dropout的方式要相同，因此使用了一个mask来记录要drop的元素。 提出一种Two-stepped fine-tuning，第一步是将模型冻结，将原GoogLeNet中的softmax层的节点数量替换为数据集类别数量，单独训练分类子网络。第二步是，解除冻结，对整个网络进行微调。 除此之外，作者还讨论了无监督迁移学习方法之间效果的比较。这里讨论的是Self-training和Co-training。无监督的迁移学习用到的数据也是多个摄像头拍摄的行人，假设我们现在有A、B两个摄像头拍下的无标签的行人图像，A摄像头下的每个行人我们都赋予一个独特的ID，然后通过最近邻搜索找到B摄像头中与之最接近的行人并附上相同的标签，这个想法很简单，但是实现起来效果很差，因为很容易存在多个A摄像头中的行人都是B摄像头中的同一个行人与之最相似，而B摄像头中的某些行人无法与A摄像头中的行人ID产生对应关系。而Co-training正好就能弥补Self-training的缺陷，主要流程是设计两个具有互补性质的模型，用模型1来标注所有的无标签数据，用模型1标注的数据来训练模型2，再用模型2来标注所有的无标签数据，用模型2标注的数据来训练模型1，循环直到所有的未标注数据都拥有标注为止。","link":"/Re：从零开始的行人重识别（二）.html"},{"title":"简单理解PRF","text":"精准率模型输出的结果是正确的概率 召回率模型中原本应该输出的结果是实际输出的概率 混淆矩阵![混淆矩阵](https://garyliu0816.github.io/images/confusion_matrix.png) 精准率$$P={TP\\over{TP+FP}}$$ 回归率$$R={TP\\over{TP+FN}}$$ F1-score$$F_1=2{PR\\over{P+R}}$$ 多说一句 精准率低相当于输出的预测很多都是错的 召回率低相当于很多应该被预测到的结果没有预测到","link":"/简单理解PRF.html"},{"title":"CADA-VAE论文阅读笔记","text":"文中涉及的概念Few-shot learning每个类只有很少的可见例子（一般小于10），训练一个模型对其分类。 Zero-shot learning现有一些可见的例子和一些不可见的例子，使用可见的例子训练模型能够对不可见的例子进行分类。 Generalized zero-shot learning现有一些可见的例子和一些不可见的例子，使用可见的例子训练模型能够对所有的例子进行分类。 Multi-modal alignment多模态对齐，寻找不同模态（如图像、文字、语音）中的属性的对应关系，文中主要是指多个VAE中的latent space的属性的对齐。 本文主要的工作贡献 训练了多个VAE从不同模态，如图像和类属性，中加密和解密特征，得到隐特征 通过对齐参数分布和减小跨模态重构损失来使隐特征多模态对齐 CADA-VAE证明了用于广义零镜头学习的交叉模态嵌入模型比数据生成方法具有更好的性能，建立了新的state-of-the-art。 主要的难点详解数据增强的方式不是增强数据本身而是增强数据的表达（隐特征） VAE变分自编码是自编码的一种，其学习得到的特征能够用于生成更多的数据。也就是我们想通过很多可观测到的X图像分布来构造出z特征，通过调整z的属性来得到更多不同的X1图像分布,所以我们的任务可以描述为： 使用X通过编码器计算得到z 通过z通过解码器计算得到X1 让q(z)与p(z|X)近可能的相似 这里的q(z)与p(z|X)是两个概率分布，通常计算两个概率分布的距离公式采用KL散度： $$D_{KL}({q(x)}\\parallel{p(x)})={\\sum p(x)\\ln\\frac{p(x)}{q(x)}}$$ 训练VAE的损失函数： $$loss_{VAE} = D_{KL}(q(X,z) \\parallel p(X,z)) = \\sum [- \\sum p(z|X)\\ln{q(X|z)} + D_{KL}({p(z|X)}\\parallel{q(z)})]$$ 模态对齐的方式这里两种模态的特征都是用VAE生成的，第一个模态的特征z_1是使用图像生成的，第二个模态的特征z_2是使用类属性生成的，要使他们的属性对齐文中采用了两种损失函数。 交叉对齐损失实际上就是将一张图像x与其类属性分别通过编码器E_1、E_2后得到的z_1、z_2交叉通过解码器D_1、D_2得到的x’计算欧式距离 $$loss_{CA} = \\sum^M_i\\sum^M_{j\\not={i}}\\left|x^{(j)}-D_j(E_i(x^{(i)}))\\right|$$ 分布对齐损失这里实际上是在计算z_1、z_2的概率分布的相似程度，采用的是Wasserstein距离： $$W_{ij}=\\left(\\left|\\mu_i-\\mu_j\\right|^2_2+\\left|\\sum\\nolimits^{1\\over2}i-\\sum\\nolimits^{1\\over2}_j\\right|^2{Frobenius}\\right)^{1\\over2}$$ 而分布对齐损失就是所有组合情况的Wasserstein距离之和： $$loss_{DA}=\\sum^M_i\\sum^M_{j\\not=i}W_{ij}$$ 整体的损失函数就是上述三个损失函数的加权求和： $$loss_{CADA-VAE}=loss_{VAE}+\\gamma loss_{CA}+\\delta loss_{DA}$$ 总结作者提出的zero-shot learning的思想主要是如何将图像通过一个VAE得到的特征与该图像对应的类属性通过另一个VAE得到的特征对应起来，这样就可以将类属性提取的特征可以与图像提取的特征进行比较，当一个未知图像进入模型后就能计算出它的特征与各个类属性的特征之间的距离，根据各个距离之间的比例就能实现zero-shot learning。","link":"/CADA-VAE论文阅读笔记.html"}],"tags":[{"name":"python","slug":"python","link":"/tags/python/"},{"name":"anaconda","slug":"anaconda","link":"/tags/anaconda/"},{"name":"deep learning","slug":"deep-learning","link":"/tags/deep-learning/"},{"name":"semi-supervised learning","slug":"semi-supervised-learning","link":"/tags/semi-supervised-learning/"},{"name":"ReID","slug":"ReID","link":"/tags/ReID/"},{"name":"markdown","slug":"markdown","link":"/tags/markdown/"},{"name":"tensorflow","slug":"tensorflow","link":"/tags/tensorflow/"},{"name":"gpu","slug":"gpu","link":"/tags/gpu/"},{"name":"cuda","slug":"cuda","link":"/tags/cuda/"},{"name":"zero-shot learning","slug":"zero-shot-learning","link":"/tags/zero-shot-learning/"},{"name":"few-shot learning","slug":"few-shot-learning","link":"/tags/few-shot-learning/"}],"categories":[{"name":"环境配置","slug":"环境配置","link":"/categories/环境配置/"},{"name":"学习笔记","slug":"学习笔记","link":"/categories/学习笔记/"},{"name":"教程","slug":"教程","link":"/categories/教程/"}]}