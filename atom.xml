<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Garyliu&#39;s Blog</title>
  
  <subtitle>Deep learner</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://garyliu0816.github.io/"/>
  <updated>2019-11-22T18:29:02.301Z</updated>
  <id>https://garyliu0816.github.io/</id>
  
  <author>
    <name>Gary Liu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Re：从零开始的行人重识别（三）</title>
    <link href="https://garyliu0816.github.io/Re%EF%BC%9A%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E7%9A%84%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%89%EF%BC%89.html"/>
    <id>https://garyliu0816.github.io/Re：从零开始的行人重识别（三）.html</id>
    <published>2019-11-22T13:00:09.000Z</published>
    <updated>2019-11-22T18:29:02.301Z</updated>
    
    <content type="html"><![CDATA[<h2 id="度量学习之对比损失"><a href="#度量学习之对比损失" class="headerlink" title="度量学习之对比损失"></a>度量学习之对比损失</h2><blockquote><p>Hadsell R, Chopra S, LeCun Y. Dimensionality reduction by learning an invariant mapping[C]//2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06). IEEE, 2006, 2: 1735-1742.</p></blockquote><p>这篇论文早在2006年就被提出来，其中最重要的核心思想就是减小类内距离，扩大类间距离。因此作者提出一个对比损失函数：<br>$$<br>L(W,Y,\vec{X_1},\vec{X_2})=(1-Y)\frac{1}{2}{(D_W)}^2+(Y)\frac{1}{2}{(\max{0,m-D_W})}^2<br>$$<br>其中，<br>$$<br>D_{W}(\vec{X_1},\vec{X_2})={\left|G_W(\vec{X_1})-G_W(\vec{X_2})\right|}_2<br>$$<br><a id="more"></a><br>这个损失函数其实可以当做两部分来看，第一部分是当\(\vec{X_1}\)与\(\vec{X_2}\)的分类或者标签又或者是身份相同时\(Y=0\)，此时的函数就变成了，<br>$$<br>L_S(W,\vec{X_1},\vec{X_2})=\frac{1}{2}{(D_W)}^2<br>$$<br>此时\(\vec{X_1}\)与\(\vec{X_2}\)的距离越小损失函数越小，否则当\(\vec{X_1}\)与\(\vec{X_2}\)的分类不同时\(Y=1\)，那么此时的函数可以看做成，<br>$$<br>L_D(W,\vec{X_1},\vec{X_2})=\frac{1}{2}{(max{ 0,m-D_W })}^2<br>$$<br>此时\(\vec{X_1}\)与\(\vec{X_2}\)的距离越大损失函数越小。下图可以很好地表示Loss值与\(D_W\)的关系，其中蓝色的线表示\(Y=1\)时的损失变化曲线，红色的线表示\(Y=0\)时的损失变化曲线：</p><p><div align="center"><img src="https://garyliu0816.github.io/images/ReID_2.png" alt="Graph of the loss function L against the energy DW"><br>通过这种方式，对模型进行训练，就会使得模型中同一类的特征相互吸引，不同类的特征相互排斥，最后达到不同类之间的距离均大于同类之间的距离。以下是该算法应用在MNIST手写数字识别数据集上的可视化效果图： </div></p><p><div align="center"><img src="https://garyliu0816.github.io/images/ReID_3.png" alt="placement of the test samples in output space"></div></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;度量学习之对比损失&quot;&gt;&lt;a href=&quot;#度量学习之对比损失&quot; class=&quot;headerlink&quot; title=&quot;度量学习之对比损失&quot;&gt;&lt;/a&gt;度量学习之对比损失&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Hadsell R, Chopra S, LeCun Y. Dimensionality reduction by learning an invariant mapping[C]//2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06). IEEE, 2006, 2: 1735-1742.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这篇论文早在2006年就被提出来，其中最重要的核心思想就是减小类内距离，扩大类间距离。因此作者提出一个对比损失函数：&lt;br&gt;$$&lt;br&gt;L(W,Y,\vec{X_1},\vec{X_2})=(1-Y)\frac{1}{2}{(D_W)}^2+(Y)\frac{1}{2}{(\max{0,m-D_W})}^2&lt;br&gt;$$&lt;br&gt;其中，&lt;br&gt;$$&lt;br&gt;D_{W}(\vec{X_1},\vec{X_2})={\left|G_W(\vec{X_1})-G_W(\vec{X_2})\right|}_2&lt;br&gt;$$&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="教程" scheme="https://garyliu0816.github.io/categories/%E6%95%99%E7%A8%8B/"/>
    
    
      <category term="ReID" scheme="https://garyliu0816.github.io/tags/ReID/"/>
    
  </entry>
  
  <entry>
    <title>Re：从零开始的行人重识别（二）</title>
    <link href="https://garyliu0816.github.io/Re%EF%BC%9A%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E7%9A%84%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%EF%BC%88%E4%BA%8C%EF%BC%89.html"/>
    <id>https://garyliu0816.github.io/Re：从零开始的行人重识别（二）.html</id>
    <published>2019-11-18T12:38:44.000Z</published>
    <updated>2019-11-18T13:54:50.074Z</updated>
    
    <content type="html"><![CDATA[<h2 id="用分类来学习行人特征"><a href="#用分类来学习行人特征" class="headerlink" title="用分类来学习行人特征"></a>用分类来学习行人特征</h2><blockquote><p>Geng M, Wang Y, Xiang T, et al. Deep transfer learning for person re-identification[J]. arXiv preprint arXiv:1611.05244, 2016.</p></blockquote><p>该论文的提出的模型是使用的ImageNet上训练好的模型进行初始化，然后对特征采用了两个子网络进行训练，一个是分类子网络，一个是验证子网络。分类子网络的输入是一张一张的图像，采用的损失函数是cross-entropy loss，这样使得类间特征的距离扩大。验证子网络的输入是成对的图像，值得注意的是这里采用的损失函数不是contrastive loss而是与之前一样的cross-entropy loss，参考作者给出的原因是他们发现用contrastive loss反而模型性能会变差，具体操作是将成对的图像特征相减，然后当做是二分类模型，输出相似或者不相似，这样能使类内特征的距离减小。<br><a id="more"></a><br>该模型的结构如下：</p><p><div align="center"><img src="https://garyliu0816.github.io/images/ReID_1.png" alt="deep Re-ID network architecture"></div></p><p>论文中的细节：</p><ol><li>base network采用的是在ImageNet上预训练过的GoogLeNet，发现效果和ResNet相当。</li><li>提出一种Loss specific dropout unit，该单元特殊用于验证子网络，因为要对两个图像进行相减操作，因此两张图像的dropout的方式要相同，因此使用了一个mask来记录要drop的元素。</li><li>提出一种Two-stepped fine-tuning，第一步是将模型冻结，将原GoogLeNet中的softmax层的节点数量替换为数据集类别数量，单独训练分类子网络。第二步是，解除冻结，对整个网络进行微调。</li></ol><p>除此之外，作者还讨论了无监督迁移学习方法之间效果的比较。这里讨论的是Self-training和Co-training。无监督的迁移学习用到的数据也是多个摄像头拍摄的行人，假设我们现在有A、B两个摄像头拍下的无标签的行人图像，A摄像头下的每个行人我们都赋予一个独特的ID，然后通过最近邻搜索找到B摄像头中与之最接近的行人并附上相同的标签，这个想法很简单，但是实现起来效果很差，因为很容易存在多个A摄像头中的行人都是B摄像头中的同一个行人与之最相似，而B摄像头中的某些行人无法与A摄像头中的行人ID产生对应关系。<br>而Co-training正好就能弥补Self-training的缺陷，主要流程是设计两个具有互补性质的模型，用模型1来标注所有的无标签数据，用模型1标注的数据来训练模型2，再用模型2来标注所有的无标签数据，用模型2标注的数据来训练模型1，循环直到所有的未标注数据都拥有标注为止。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;用分类来学习行人特征&quot;&gt;&lt;a href=&quot;#用分类来学习行人特征&quot; class=&quot;headerlink&quot; title=&quot;用分类来学习行人特征&quot;&gt;&lt;/a&gt;用分类来学习行人特征&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Geng M, Wang Y, Xiang T, et al. Deep transfer learning for person re-identification[J]. arXiv preprint arXiv:1611.05244, 2016.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;该论文的提出的模型是使用的ImageNet上训练好的模型进行初始化，然后对特征采用了两个子网络进行训练，一个是分类子网络，一个是验证子网络。分类子网络的输入是一张一张的图像，采用的损失函数是cross-entropy loss，这样使得类间特征的距离扩大。验证子网络的输入是成对的图像，值得注意的是这里采用的损失函数不是contrastive loss而是与之前一样的cross-entropy loss，参考作者给出的原因是他们发现用contrastive loss反而模型性能会变差，具体操作是将成对的图像特征相减，然后当做是二分类模型，输出相似或者不相似，这样能使类内特征的距离减小。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="教程" scheme="https://garyliu0816.github.io/categories/%E6%95%99%E7%A8%8B/"/>
    
    
      <category term="ReID" scheme="https://garyliu0816.github.io/tags/ReID/"/>
    
  </entry>
  
  <entry>
    <title>Re：从零开始的行人重识别（一）</title>
    <link href="https://garyliu0816.github.io/Re%EF%BC%9A%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E7%9A%84%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%80%EF%BC%89.html"/>
    <id>https://garyliu0816.github.io/Re：从零开始的行人重识别（一）.html</id>
    <published>2019-11-17T09:56:18.000Z</published>
    <updated>2019-11-22T14:25:00.804Z</updated>
    
    <content type="html"><![CDATA[<h2 id="绪言"><a href="#绪言" class="headerlink" title="绪言"></a>绪言</h2><p>马上研二要开题了，提前通过blog的方式先把研究思路理清楚，也算是重新温习一下学习过程中遇到的问题吧，希望能给自己带来一些新的思路，同时帮助一些刚刚接触reID的同学入门（<del>虽然应该没什么人会看</del>），如有勘误可以提出来，我会及时改正的。<br><a id="more"></a></p><h2 id="什么是行人重识别？"><a href="#什么是行人重识别？" class="headerlink" title="什么是行人重识别？"></a>什么是行人重识别？</h2><p>行人重识别的是属于计算机视觉下的重要的也是当前比较困难的任务。它主要要解决问题是如何在不同的摄像头下找到相同的一个人，这种问题其实可以看做是一个图像检索的问题，即是通过一张行人在一个摄像头下的图像（或视频）作为检索目标，在其余摄像头中找到与之最相似的行人。</p><h2 id="为什么需要行人重识别？"><a href="#为什么需要行人重识别？" class="headerlink" title="为什么需要行人重识别？"></a>为什么需要行人重识别？</h2><p>主要是由于当前用于行人检测的摄像头大多是固定位置的局限性，当行人离开摄像头后，就无法继续追踪新摄像头下的行人，当行人重识别与行人检测技术相结合后，就可以弥补缺陷。行人重识别广泛应用于智能视频监控、智能安保等领域。</p><h2 id="行人重识别问题的难点"><a href="#行人重识别问题的难点" class="headerlink" title="行人重识别问题的难点"></a>行人重识别问题的难点</h2><ol><li>在监控摄像头下的人脸往往模糊无法看清，因此不能直接使用人脸的信息作为识别的特征。</li><li>不同的摄像头下因光照、行人的尺寸、行人的姿态，行人的朝向的不同，导致行人不能只靠颜色、轮廓等信息进行重识别。</li><li>当前的行人重识别模型的泛化能力还有所欠缺，在一个数据集上训练的数据无法在另一个数据集上也有较好的表现。</li></ol><h2 id="行人重识别常用数据集介绍"><a href="#行人重识别常用数据集介绍" class="headerlink" title="行人重识别常用数据集介绍"></a>行人重识别常用数据集介绍</h2><h3 id="Market1501"><a href="#Market1501" class="headerlink" title="Market1501"></a>Market1501</h3><blockquote><p>Zheng L, Shen L, Tian L, et al. Scalable person re-identification: A benchmark[C]//Proceedings of the IEEE international conference on computer vision. 2015: 1116-1124.</p></blockquote><p>2015年清华大学整理并提出，采集自6个摄像头（其中5个高清摄像头和1个低清摄像头）。Market1501包含了12,936张训练（train）图像，3,368张查询（query）图像，以及19,732张图库（gallery）图像，数据集包含1501个ID，其中训练集包含751个ID的图像，图库集包含750个ID的图像。</p><p><a href="https://pan.baidu.com/s/1ntIi2Op" target="_blank" rel="noopener">下载链接</a></p><h3 id="DukeMTMC-reID"><a href="#DukeMTMC-reID" class="headerlink" title="DukeMTMC-reID"></a>DukeMTMC-reID</h3><blockquote><p>Ristani E, Solera F, Zou R, et al. Performance measures and a data set for multi-target, multi-camera tracking[C]//European Conference on Computer Vision. Springer, Cham, 2016: 17-35.</p></blockquote><p>2017年Duke大学提出，采集自8个不同的摄像头，是DukeMTMC数据集的子集，DukeMTMC-reID包含了16,522张训练图像，2,228个查询图像，以及17,661张图库图像，数据集包含1404个ID，其中训练集包含702个ID的图像，图库集包含702个ID的图像。</p><p><a href="https://pan.baidu.com/s/1jS0XM7Var5nQGcbf9xUztw" target="_blank" rel="noopener">下载链接</a>提取码：bhbh</p><h3 id="CUHK03"><a href="#CUHK03" class="headerlink" title="CUHK03"></a>CUHK03</h3><blockquote><p>Li W, Zhao R, Xiao T, et al. Deepreid: Deep filter pairing neural network for person re-identification[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2014: 152-159.</p></blockquote><p>2014年香港中文大学大学提出，采集自10个不同的摄像头，CUHK03包含了13164 张图像，数据集包含1360个ID，其中有1160个ID属于训练集，100个ID属于验证集，100个ID属于测试集。</p><p><a href="pan.baidu.com/s/1mgklxSc">下载链接</a>提取码：rhjq</p><h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><h3 id="Rank-n"><a href="#Rank-n" class="headerlink" title="Rank-n"></a>Rank-n</h3><p>搜索结果中top-n张图有正确结果的概率。</p><p>例如： query image为 <strong>m1</strong> ，在大小为100的gallery中搜索。</p><p>如果识别结果是 <strong>m1</strong> 、m2、m3、m4、m5……，则此时rank-1的正确率为100%，rank-2的正确率也为100%，rank-5的正确率也为100%；<br>如果识别结果是m2、 <strong>m1</strong> 、m3、m4、m5……，则此时rank-1的正确率为0%，rank-2的正确率为100%，rank-5的正确率也为100%；<br>如果识别结果是m2、m3、m4、m5、 <strong>m1</strong> ……，则此时rank-1的正确率为0%，rank-2的正确率为0%，rank-5的正确率为100%<br>最后求得多个query的Rank-n取平均值。</p><h3 id="CMC"><a href="#CMC" class="headerlink" title="CMC"></a>CMC</h3><p>全称为Cumulative Match Characteristic，将一个摄像头采集的图像用作查询集，将其余摄影机采集的图像用作图库集。对于图库集，为每个ID随机采样一张图像。对于查询集，将使用所有图像，获取每个图像的CMC曲线，这里的CMC曲线就是Rank-n与Accuracy的关系曲线，然后对它们进行平均。重复此评估过程100次，并将平均值提交为最终结果。</p><h3 id="mAP"><a href="#mAP" class="headerlink" title="mAP"></a>mAP</h3><p>能够反应query image在gallery中所有正确的图片排在检索列表前面的程度，能更加全面的衡量ReID算法的性能。假设query image在gallery中有4张待查询图像，在检索的列表中排序分别为1、2、5、7，则ap为（1/1 + 2/2 + 3/5 + 4/7）/4=0.793。ap较大时，检索结果都相对靠前，最后对所有query的ap取平均值得到mAP。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;绪言&quot;&gt;&lt;a href=&quot;#绪言&quot; class=&quot;headerlink&quot; title=&quot;绪言&quot;&gt;&lt;/a&gt;绪言&lt;/h2&gt;&lt;p&gt;马上研二要开题了，提前通过blog的方式先把研究思路理清楚，也算是重新温习一下学习过程中遇到的问题吧，希望能给自己带来一些新的思路，同时帮助一些刚刚接触reID的同学入门（&lt;del&gt;虽然应该没什么人会看&lt;/del&gt;），如有勘误可以提出来，我会及时改正的。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="教程" scheme="https://garyliu0816.github.io/categories/%E6%95%99%E7%A8%8B/"/>
    
    
      <category term="ReID" scheme="https://garyliu0816.github.io/tags/ReID/"/>
    
  </entry>
  
  <entry>
    <title>Good Semi-supervised learning That requires a Bad GAN论文阅读笔记</title>
    <link href="https://garyliu0816.github.io/Good%20Semi-supervised%20learning%20That%20requires%20a%20Bad%20GAN%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0.html"/>
    <id>https://garyliu0816.github.io/Good Semi-supervised learning That requires a Bad GAN论文阅读笔记.html</id>
    <published>2019-05-22T02:26:07.000Z</published>
    <updated>2019-05-22T03:33:31.590Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h2><p>GAN（生成对抗网络）在semi-supervised learning（半监督学习）上取得了较强的实证成果，但是有两点是<strong>我们都没搞明白的</strong>：</p><ol><li>discriminator（判别器）是如何从与generator（生成器）的联合训练中收益的</li><li>为什么一个好的classification（分类）效果和一个好的生成器不能同时获得</li></ol><h3 id="本质目标"><a href="#本质目标" class="headerlink" title="本质目标"></a>本质目标</h3><p>为了更好的寻找能正确分类的low-density boundary（低密度边界）<br><a id="more"></a></p><p><div align="center"><img src="https://garyliu0816.github.io/images/bad_gan_1.png" alt="low-density boundary"><br>为了防止非正确分类低密度边界的干扰，我们需要用生成器产生可见样本空间中的“补样”，填充一些低密度区域，此时的生成器产生的样本分布与真实分布是不一致的，因此生成器是“bad”的。</div></p><p>OpenAI提出的基于GAN的半监督学习的优化目标（损失函数）如下：</p><p><div align="center"><img src="https://garyliu0816.github.io/images/bad_gan_2.png" alt="semi-supervised-gan"></div></p><h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h3><ul><li>标注集L={(x,y)}，其中x为图像数据，{1,2,…,K}对应K个真类，还有一个K+1的假类</li><li>P_G、P_D是对应于G（生成器）和D（判别器）的概率分布，P_D覆盖K+1个类的分布，前K个类是真类，第K+1个类是假类（生成器生成的），和CatGAN一样</li><li>P是真实数据的概率分布</li></ul><p>这个损失函数由三部分组成。第一部分是最大化在标注集L中log下的P_D条件概率的数学期望，这个是基本的监督学习的损失函数；第二部分是最大化在真实数据p中log下的P_D对前K个类的概率的数学期望；第三部分是最大化在生成数据P_G中log下P_D对第K+1类概率的数学期望。</p><p>这样看还是太复杂了吧，我这里用一些简单的语言来<strong>解释</strong>：</p><p>第一部分，是希望D能从标注数据L中接收任意输入（x，y），得到的输出y^与输入时的y一致，也就是正确分类，然后将这种概率最大化。</p><p>第二部分，是希望D能从真实数据P（包含未标注数据）中接收任意输入x，得到的输出y被认为是真类，将这种概率最大化。</p><p>第三部分，是希望D能从生成数据P_G（都是假的）中接收任意输入x，得到的输出y被认为是假类，将这种概率最大化。<br>这里的D实际上就是一个多分类神经网络，D的最后一层就是一个softmax激活函数。</p><h2 id="理论推导"><a href="#理论推导" class="headerlink" title="理论推导"></a>理论推导</h2><p><div align="center"><img src="https://garyliu0816.github.io/images/bad_gan_3.png" alt="proposition1"></div></p><h3 id="提议1"><a href="#提议1" class="headerlink" title="提议1"></a>提议1</h3><p>如果生成器D是完美的，即P_D与P的分布完全一致，那么公式的第二部分和第三部分就是矛盾的，如果第二部分大，则第三部分必然很小，反之亦然。</p><p>因此，当生成器是完美的时候，不仅生成器不起作用，而且未标记的数据也没有得到有效利用。</p><p><div align="center"><img src="https://garyliu0816.github.io/images/bad_gan_4.png" alt="assumption1"></div></p><h3 id="假设1"><a href="#假设1" class="headerlink" title="假设1"></a>假设1</h3><p>收敛条件。当D收敛于一个训练集{L,U,G}（L是标注数据，U是未标注数据，G是生成数据），D能学到一个完全能正确分类的决策边界。更具体来说，（1）L中的数据均能正确分类；（2）G中的数据均被分在第K+1类（3）U中的数据均被分在前K个类中。</p><p><div align="center"><img src="https://garyliu0816.github.io/images/bad_gan_5.png" alt="lemma1"></div></p><p><div align="center"><img src="https://garyliu0816.github.io/images/bad_gan_6.png" alt="corollary1"></div></p><h3 id="推论1"><a href="#推论1" class="headerlink" title="推论1"></a>推论1</h3><p>当生成的数据是无穷多的时候，根据Lemma1，我们有以下推论，D在softmax前输出的属于前K个类的值均小于等于0</p><p><div align="center"><img src="https://garyliu0816.github.io/images/bad_gan_7.png" alt="proposition2"></div></p><h3 id="提议2"><a href="#提议2" class="headerlink" title="提议2"></a>提议2</h3><p>在满足推论1的情况下，我们所有的分类集合均正确分类。<br>直观地说，生成器生成补码样本，因此真实类的logits在补码样本中必须很低。因此，该鉴别器在低密度区域获得了类边界。</p><p><div align="center"><img src="https://garyliu0816.github.io/images/bad_gan_8.png" alt="pic1" title="黑色为真实数据，白色为稀疏区域（此图为上帝视角的Ground Truth）"></div></p><p><div align="center"><img src="https://garyliu0816.github.io/images/bad_gan_9.png" alt="pic2" title="黑色为生成数据，白色为稀疏区域"></div></p><h2 id="实验方法"><a href="#实验方法" class="headerlink" title="实验方法"></a>实验方法</h2><h3 id="Generator-Entropy（生成熵）"><a href="#Generator-Entropy（生成熵）" class="headerlink" title="Generator Entropy（生成熵）"></a>Generator Entropy（生成熵）</h3><p><div align="center"><img src="https://garyliu0816.github.io/images/bad_gan_10.png" alt="Generator Entropy"><br>这里可以用L_PT代替</div></p><p><div align="center"><img src="https://garyliu0816.github.io/images/bad_gan_11.png" alt="L_PT"></div></p><h3 id="Generating-Low-Density-Samples（生成低密度样本）"><a href="#Generating-Low-Density-Samples（生成低密度样本）" class="headerlink" title="Generating Low-Density Samples（生成低密度样本）"></a>Generating Low-Density Samples（生成低密度样本）</h3><p>最小化它</p><p><div align="center"><img src="https://garyliu0816.github.io/images/bad_gan_12.png" alt="Generating Low-Density Samples"></div></p><h3 id="生成器的目标函数（损失函数）"><a href="#生成器的目标函数（损失函数）" class="headerlink" title="生成器的目标函数（损失函数）"></a>生成器的目标函数（损失函数）</h3><p>也就是生成熵+生成低密度样本</p><p><div align="center"><img src="https://garyliu0816.github.io/images/bad_gan_13.png" alt="loss1"></div></p><h3 id="判别器的目标函数（损失函数）"><a href="#判别器的目标函数（损失函数）" class="headerlink" title="判别器的目标函数（损失函数）"></a>判别器的目标函数（损失函数）</h3><p><div align="center"><img src="https://garyliu0816.github.io/images/bad_gan_14.png" alt="loss2"></div></p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>当然是SOTA啦！</p><p><div align="center"><img src="https://garyliu0816.github.io/images/bad_gan_15.png" alt="result"></div></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前置知识&quot;&gt;&lt;a href=&quot;#前置知识&quot; class=&quot;headerlink&quot; title=&quot;前置知识&quot;&gt;&lt;/a&gt;前置知识&lt;/h2&gt;&lt;p&gt;GAN（生成对抗网络）在semi-supervised learning（半监督学习）上取得了较强的实证成果，但是有两点是&lt;strong&gt;我们都没搞明白的&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;discriminator（判别器）是如何从与generator（生成器）的联合训练中收益的&lt;/li&gt;
&lt;li&gt;为什么一个好的classification（分类）效果和一个好的生成器不能同时获得&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;本质目标&quot;&gt;&lt;a href=&quot;#本质目标&quot; class=&quot;headerlink&quot; title=&quot;本质目标&quot;&gt;&lt;/a&gt;本质目标&lt;/h3&gt;&lt;p&gt;为了更好的寻找能正确分类的low-density boundary（低密度边界）&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://garyliu0816.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="deep learning" scheme="https://garyliu0816.github.io/tags/deep-learning/"/>
    
      <category term="semi-supervised learning" scheme="https://garyliu0816.github.io/tags/semi-supervised-learning/"/>
    
  </entry>
  
  <entry>
    <title>CADA-VAE论文阅读笔记</title>
    <link href="https://garyliu0816.github.io/CADA-VAE%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0.html"/>
    <id>https://garyliu0816.github.io/CADA-VAE论文阅读笔记.html</id>
    <published>2019-05-07T10:42:31.000Z</published>
    <updated>2019-05-22T02:57:21.511Z</updated>
    
    <content type="html"><![CDATA[<h2 id="文中涉及的概念"><a href="#文中涉及的概念" class="headerlink" title="文中涉及的概念"></a>文中涉及的概念</h2><h3 id="Few-shot-learning"><a href="#Few-shot-learning" class="headerlink" title="Few-shot learning"></a>Few-shot learning</h3><p>每个类只有很少的可见例子（一般小于10），训练一个模型对其分类。</p><h3 id="Zero-shot-learning"><a href="#Zero-shot-learning" class="headerlink" title="Zero-shot learning"></a>Zero-shot learning</h3><p>现有一些可见的例子和一些不可见的例子，使用可见的例子训练模型能够对不可见的例子进行分类。</p><h3 id="Generalized-zero-shot-learning"><a href="#Generalized-zero-shot-learning" class="headerlink" title="Generalized zero-shot learning"></a>Generalized zero-shot learning</h3><p>现有一些可见的例子和一些不可见的例子，使用可见的例子训练模型能够对所有的例子进行分类。</p><h3 id="Multi-modal-alignment"><a href="#Multi-modal-alignment" class="headerlink" title="Multi-modal alignment"></a>Multi-modal alignment</h3><p>多模态对齐，寻找不同模态（如图像、文字、语音）中的属性的对应关系，文中主要是指多个VAE中的latent space的属性的对齐。<br><a id="more"></a></p><h2 id="本文主要的工作贡献"><a href="#本文主要的工作贡献" class="headerlink" title="本文主要的工作贡献"></a>本文主要的工作贡献</h2><ul><li>训练了多个VAE从不同模态，如图像和类属性，中加密和解密特征，得到隐特征</li><li>通过对齐参数分布和减小跨模态重构损失来使隐特征多模态对齐</li><li>CADA-VAE证明了用于广义零镜头学习的交叉模态嵌入模型比数据生成方法具有更好的性能，建立了新的state-of-the-art。</li></ul><h2 id="主要的难点详解"><a href="#主要的难点详解" class="headerlink" title="主要的难点详解"></a>主要的难点详解</h2><h3 id="数据增强的方式"><a href="#数据增强的方式" class="headerlink" title="数据增强的方式"></a>数据增强的方式</h3><p>不是增强数据本身而是增强数据的表达（隐特征）</p><h4 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h4><p>变分自编码是自编码的一种，其学习得到的特征能够用于生成更多的数据。也就是我们想通过很多可观测到的X图像分布来构造出z特征，通过调整z的属性来得到更多不同的X1图像分布,所以我们的任务可以描述为：</p><blockquote><ol><li>使用X通过编码器计算得到z</li><li>通过z通过解码器计算得到X1</li><li>让q(z)与p(z|X)近可能的相似</li></ol></blockquote><p>这里的q(z)与p(z|X)是两个概率分布，通常计算两个概率分布的距离公式采用KL散度：</p><p>$$D_{KL}({q(x)}\parallel{p(x)})={\sum p(x)\ln\frac{p(x)}{q(x)}}$$</p><p>训练VAE的损失函数：</p><p>$$loss_{VAE} = D_{KL}(q(X,z) \parallel p(X,z)) = \sum [- \sum p(z|X)\ln{q(X|z)} + D_{KL}({p(z|X)}\parallel{q(z)})]$$</p><h3 id="模态对齐的方式"><a href="#模态对齐的方式" class="headerlink" title="模态对齐的方式"></a>模态对齐的方式</h3><p>这里两种模态的特征都是用VAE生成的，第一个模态的特征z_1是使用图像生成的，第二个模态的特征z_2是使用类属性生成的，要使他们的属性对齐文中采用了两种损失函数。</p><h4 id="交叉对齐损失"><a href="#交叉对齐损失" class="headerlink" title="交叉对齐损失"></a>交叉对齐损失</h4><p>实际上就是将一张图像x与其类属性分别通过编码器E_1、E_2后得到的z_1、z_2交叉通过解码器D_1、D_2得到的x’计算欧式距离</p><p>$$loss_{CA} = \sum^M_i\sum^M_{j\not={i}}\left|x^{(j)}-D_j(E_i(x^{(i)}))\right|$$</p><h4 id="分布对齐损失"><a href="#分布对齐损失" class="headerlink" title="分布对齐损失"></a>分布对齐损失</h4><p>这里实际上是在计算z_1、z_2的概率分布的相似程度，采用的是Wasserstein距离：</p><p>$$<br>W_{ij}=\left(\left|\mu_i-\mu_j\right|^2_2+\left|\sum\nolimits^{1\over2}_i-\sum\nolimits^{1\over2}<em>j\right|^2</em>{Frobenius}\right)^{1\over2}<br>$$</p><p>而分布对齐损失就是所有组合情况的Wasserstein距离之和：</p><p>$$loss_{DA}=\sum^M_i\sum^M_{j\not=i}W_{ij}$$</p><h3 id="整体的损失函数"><a href="#整体的损失函数" class="headerlink" title="整体的损失函数"></a>整体的损失函数</h3><p>就是上述三个损失函数的加权求和：</p><p>$$loss_{CADA-VAE}=loss_{VAE}+\gamma loss_{CA}+\delta loss_{DA}$$</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>作者提出的zero-shot learning的思想主要是如何将图像通过一个VAE得到的特征与该图像对应的类属性通过另一个VAE得到的特征对应起来，这样就可以将类属性提取的特征可以与图像提取的特征进行比较，当一个未知图像进入模型后就能计算出它的特征与各个类属性的特征之间的距离，根据各个距离之间的比例就能实现zero-shot learning。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;文中涉及的概念&quot;&gt;&lt;a href=&quot;#文中涉及的概念&quot; class=&quot;headerlink&quot; title=&quot;文中涉及的概念&quot;&gt;&lt;/a&gt;文中涉及的概念&lt;/h2&gt;&lt;h3 id=&quot;Few-shot-learning&quot;&gt;&lt;a href=&quot;#Few-shot-learning&quot; class=&quot;headerlink&quot; title=&quot;Few-shot learning&quot;&gt;&lt;/a&gt;Few-shot learning&lt;/h3&gt;&lt;p&gt;每个类只有很少的可见例子（一般小于10），训练一个模型对其分类。&lt;/p&gt;
&lt;h3 id=&quot;Zero-shot-learning&quot;&gt;&lt;a href=&quot;#Zero-shot-learning&quot; class=&quot;headerlink&quot; title=&quot;Zero-shot learning&quot;&gt;&lt;/a&gt;Zero-shot learning&lt;/h3&gt;&lt;p&gt;现有一些可见的例子和一些不可见的例子，使用可见的例子训练模型能够对不可见的例子进行分类。&lt;/p&gt;
&lt;h3 id=&quot;Generalized-zero-shot-learning&quot;&gt;&lt;a href=&quot;#Generalized-zero-shot-learning&quot; class=&quot;headerlink&quot; title=&quot;Generalized zero-shot learning&quot;&gt;&lt;/a&gt;Generalized zero-shot learning&lt;/h3&gt;&lt;p&gt;现有一些可见的例子和一些不可见的例子，使用可见的例子训练模型能够对所有的例子进行分类。&lt;/p&gt;
&lt;h3 id=&quot;Multi-modal-alignment&quot;&gt;&lt;a href=&quot;#Multi-modal-alignment&quot; class=&quot;headerlink&quot; title=&quot;Multi-modal alignment&quot;&gt;&lt;/a&gt;Multi-modal alignment&lt;/h3&gt;&lt;p&gt;多模态对齐，寻找不同模态（如图像、文字、语音）中的属性的对应关系，文中主要是指多个VAE中的latent space的属性的对齐。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://garyliu0816.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="deep learning" scheme="https://garyliu0816.github.io/tags/deep-learning/"/>
    
      <category term="zero-shot learning" scheme="https://garyliu0816.github.io/tags/zero-shot-learning/"/>
    
      <category term="few-shot learning" scheme="https://garyliu0816.github.io/tags/few-shot-learning/"/>
    
  </entry>
  
  <entry>
    <title>简单理解PRF</title>
    <link href="https://garyliu0816.github.io/%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3PRF.html"/>
    <id>https://garyliu0816.github.io/简单理解PRF.html</id>
    <published>2019-03-30T06:29:17.000Z</published>
    <updated>2019-07-25T08:07:14.637Z</updated>
    
    <content type="html"><![CDATA[<h2 id="精准率"><a href="#精准率" class="headerlink" title="精准率"></a>精准率</h2><p>模型输出的结果是正确的概率</p><h2 id="召回率"><a href="#召回率" class="headerlink" title="召回率"></a>召回率</h2><p>模型中原本应该输出的结果是实际输出的概率</p><h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><p><div align="center"><img src="https://garyliu0816.github.io/images/confusion_matrix.png" alt="混淆矩阵"><br><a id="more"></a></div></p><h3 id="精准率-1"><a href="#精准率-1" class="headerlink" title="精准率"></a>精准率</h3><p>$$P={TP\over{TP+FP}}$$</p><h3 id="回归率"><a href="#回归率" class="headerlink" title="回归率"></a>回归率</h3><p>$$R={TP\over{TP+FN}}$$</p><h3 id="F1-score"><a href="#F1-score" class="headerlink" title="F1-score"></a>F1-score</h3><p>$$F_1=2{PR\over{P+R}}$$</p><h2 id="多说一句"><a href="#多说一句" class="headerlink" title="多说一句"></a>多说一句</h2><ul><li>精准率低相当于输出的预测很多都是错的</li><li>召回率低相当于很多应该被预测到的结果没有预测到</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;精准率&quot;&gt;&lt;a href=&quot;#精准率&quot; class=&quot;headerlink&quot; title=&quot;精准率&quot;&gt;&lt;/a&gt;精准率&lt;/h2&gt;&lt;p&gt;模型输出的结果是正确的概率&lt;/p&gt;
&lt;h2 id=&quot;召回率&quot;&gt;&lt;a href=&quot;#召回率&quot; class=&quot;headerlink&quot; title=&quot;召回率&quot;&gt;&lt;/a&gt;召回率&lt;/h2&gt;&lt;p&gt;模型中原本应该输出的结果是实际输出的概率&lt;/p&gt;
&lt;h2 id=&quot;混淆矩阵&quot;&gt;&lt;a href=&quot;#混淆矩阵&quot; class=&quot;headerlink&quot; title=&quot;混淆矩阵&quot;&gt;&lt;/a&gt;混淆矩阵&lt;/h2&gt;&lt;p&gt;&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://garyliu0816.github.io/images/confusion_matrix.png&quot; alt=&quot;混淆矩阵&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://garyliu0816.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="deep learning" scheme="https://garyliu0816.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow+cuda+cudnn安装成功的版本（持续更新）</title>
    <link href="https://garyliu0816.github.io/tensorflow-cuda-cudnn%E5%AE%89%E8%A3%85%E6%88%90%E5%8A%9F%E7%9A%84%E7%89%88%E6%9C%AC%EF%BC%88%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%EF%BC%89.html"/>
    <id>https://garyliu0816.github.io/tensorflow-cuda-cudnn安装成功的版本（持续更新）.html</id>
    <published>2019-03-27T13:41:37.000Z</published>
    <updated>2019-11-22T14:16:46.287Z</updated>
    
    <content type="html"><![CDATA[<h2 id="以下版本经过本人亲自测试可以安装使用"><a href="#以下版本经过本人亲自测试可以安装使用" class="headerlink" title="以下版本经过本人亲自测试可以安装使用"></a>以下版本经过本人亲自测试可以安装使用</h2><table><thead><tr><th>tensorflow</th><th style="text-align:right">cuda</th><th style="text-align:right">python</th><th style="text-align:right">cudnn</th><th style="text-align:right">visual C++ redistributable</th><th style="text-align:right">build tools</th></tr></thead><tbody><tr><td>gpu-1.13.1</td><td style="text-align:right">10.1</td><td style="text-align:right">3.7</td><td style="text-align:right">7</td><td style="text-align:right">2015 update 3</td><td style="text-align:right">2015 Update 3</td></tr><tr><td>gpu-1.13.1</td><td style="text-align:right">10.0</td><td style="text-align:right">3.6</td><td style="text-align:right">7</td><td style="text-align:right">2015 update 3</td><td style="text-align:right">2015 Update 3</td></tr><tr><td>gpu-1.12.0</td><td style="text-align:right">9.0</td><td style="text-align:right">3.5</td><td style="text-align:right">7</td><td style="text-align:right">2015 update 3</td><td style="text-align:right">2015 Update 3</td></tr></tbody></table><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;以下版本经过本人亲自测试可以安装使用&quot;&gt;&lt;a href=&quot;#以下版本经过本人亲自测试可以安装使用&quot; class=&quot;headerlink&quot; title=&quot;以下版本经过本人亲自测试可以安装使用&quot;&gt;&lt;/a&gt;以下版本经过本人亲自测试可以安装使用&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;tensorflow&lt;/th&gt;
&lt;th style=&quot;text-align:right&quot;&gt;cuda&lt;/th&gt;
&lt;th style=&quot;text-align:right&quot;&gt;python&lt;/th&gt;
&lt;th style=&quot;text-align:right&quot;&gt;cudnn&lt;/th&gt;
&lt;th style=&quot;text-align:right&quot;&gt;visual C++ redistributable&lt;/th&gt;
&lt;th style=&quot;text-align:right&quot;&gt;build tools&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;gpu-1.13.1&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;10.1&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;3.7&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;7&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;2015 update 3&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;2015 Update 3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;gpu-1.13.1&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;10.0&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;3.6&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;7&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;2015 update 3&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;2015 Update 3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;gpu-1.12.0&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;9.0&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;3.5&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;7&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;2015 update 3&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;2015 Update 3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
    
    </summary>
    
      <category term="环境配置" scheme="https://garyliu0816.github.io/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="tensorflow" scheme="https://garyliu0816.github.io/tags/tensorflow/"/>
    
      <category term="gpu" scheme="https://garyliu0816.github.io/tags/gpu/"/>
    
      <category term="cuda" scheme="https://garyliu0816.github.io/tags/cuda/"/>
    
  </entry>
  
  <entry>
    <title>Anaconda常用指令</title>
    <link href="https://garyliu0816.github.io/Anaconda%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4.html"/>
    <id>https://garyliu0816.github.io/Anaconda常用指令.html</id>
    <published>2019-03-26T12:01:06.000Z</published>
    <updated>2019-03-30T06:19:56.940Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-查看包"><a href="#1-查看包" class="headerlink" title="1. 查看包"></a>1. 查看包</h2><figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">conda list     <span class="hljs-comment"># 查看安装了哪些包</span></span><br><span class="line">conda env list <span class="hljs-comment"># 查看有哪些虚拟环境</span></span><br><span class="line">conda -V       <span class="hljs-comment"># 查看conda的版本</span></span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="2-创建虚拟环境"><a href="#2-创建虚拟环境" class="headerlink" title="2. 创建虚拟环境"></a>2. 创建虚拟环境</h2><figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">conda create -n [虚拟环境名] python=[python版本] [要安装的包...] </span><br><span class="line">conda create -n python3 python=3.6 numpy     <span class="hljs-comment"># 创建一个名为python3，python版本为3.6，安装了numpy的环境</span></span><br><span class="line">conda create -n backup --<span class="hljs-built_in">clone</span> root     <span class="hljs-comment"># 创建一个名为backup的与root一样的环境</span></span><br></pre></td></tr></table></figure><h2 id="3-切换环境"><a href="#3-切换环境" class="headerlink" title="3. 切换环境"></a>3. 切换环境</h2><figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-built_in">source</span> activate [环境名] <span class="hljs-comment"># Linux</span></span><br><span class="line">activate [环境名]        <span class="hljs-comment"># Windows</span></span><br></pre></td></tr></table></figure><h2 id="4-关闭环境"><a href="#4-关闭环境" class="headerlink" title="4. 关闭环境"></a>4. 关闭环境</h2><figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-built_in">source</span> deactivate <span class="hljs-comment"># Linux</span></span><br><span class="line">deactivate        <span class="hljs-comment"># Windows</span></span><br></pre></td></tr></table></figure><h2 id="5-指定虚拟环境安装包"><a href="#5-指定虚拟环境安装包" class="headerlink" title="5. 指定虚拟环境安装包"></a>5. 指定虚拟环境安装包</h2><figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">conda install -n [虚拟环境名] [包名]</span><br><span class="line"><span class="hljs-comment"># 或者可以先进入对应虚拟环境在执行pip</span></span><br><span class="line">pip install [包名]</span><br></pre></td></tr></table></figure><h2 id="6-移除虚拟环境"><a href="#6-移除虚拟环境" class="headerlink" title="6. 移除虚拟环境"></a>6. 移除虚拟环境</h2><figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">conda remove --name [虚拟环境名] [包名]         <span class="hljs-comment"># 移除某个环境中的包</span></span><br><span class="line">conda remove -n [虚拟环境名] --all  <span class="hljs-comment"># 移除某个虚拟环境</span></span><br></pre></td></tr></table></figure><h2 id="7-将环境备份至requirement-txt"><a href="#7-将环境备份至requirement-txt" class="headerlink" title="7. 将环境备份至requirement.txt"></a>7. 将环境备份至requirement.txt</h2><figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">pip freeze &gt; requirements.txt</span><br></pre></td></tr></table></figure><h2 id="8-安装备份的环境"><a href="#8-安装备份的环境" class="headerlink" title="8. 安装备份的环境"></a>8. 安装备份的环境</h2><figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-查看包&quot;&gt;&lt;a href=&quot;#1-查看包&quot; class=&quot;headerlink&quot; title=&quot;1. 查看包&quot;&gt;&lt;/a&gt;1. 查看包&lt;/h2&gt;&lt;figure class=&quot;highlight bash hljs&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;conda list     &lt;span class=&quot;hljs-comment&quot;&gt;# 查看安装了哪些包&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conda env list &lt;span class=&quot;hljs-comment&quot;&gt;# 查看有哪些虚拟环境&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conda -V       &lt;span class=&quot;hljs-comment&quot;&gt;# 查看conda的版本&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="环境配置" scheme="https://garyliu0816.github.io/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="python" scheme="https://garyliu0816.github.io/tags/python/"/>
    
      <category term="anaconda" scheme="https://garyliu0816.github.io/tags/anaconda/"/>
    
  </entry>
  
  <entry>
    <title>Markdown常用标记</title>
    <link href="https://garyliu0816.github.io/Markdown%E5%B8%B8%E7%94%A8%E6%A0%87%E8%AE%B0.html"/>
    <id>https://garyliu0816.github.io/Markdown常用标记.html</id>
    <published>2019-03-26T11:16:21.000Z</published>
    <updated>2019-04-01T07:35:57.286Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-标题"><a href="#1-标题" class="headerlink" title="1. 标题"></a>1. 标题</h2><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line"># 一级标题</span><br><span class="line">## 二级标题</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><h2 id="2-分割线"><a href="#2-分割线" class="headerlink" title="2. 分割线"></a>2. 分割线</h2><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">---</span><br></pre></td></tr></table></figure><hr><h2 id="3-引用"><a href="#3-引用" class="headerlink" title="3. 引用"></a>3. 引用</h2><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">&gt;引用</span><br><span class="line">&gt;&gt;嵌套引用</span><br></pre></td></tr></table></figure><blockquote><p>引用</p><blockquote><p>嵌套引用</p></blockquote></blockquote><h2 id="4-字体"><a href="#4-字体" class="headerlink" title="4. 字体"></a>4. 字体</h2><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">*倾斜*</span><br><span class="line">**粗体**</span><br><span class="line">***倾斜加粗***</span><br><span class="line">~~删除线~~</span><br></pre></td></tr></table></figure><p><em>倾斜</em><br><strong>粗体</strong><br><strong><em>倾斜加粗</em></strong><br><del>删除线</del></p><h2 id="5-超链接"><a href="#5-超链接" class="headerlink" title="5. 超链接"></a>5. 超链接</h2><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">[超链接名](超链接地址 &quot;超链接title&quot;)</span><br><span class="line">![图片名](图片地址 &quot;图片title&quot;)</span><br></pre></td></tr></table></figure><p><a href="https://cn.bing.com" title="必应" target="_blank" rel="noopener">必应</a><br><img src="https://tensorfly.cn/images/tensors_flowing.gif" alt="数据流图" title="数据流图"></p><h2 id="6-代码"><a href="#6-代码" class="headerlink" title="6. 代码"></a>6. 代码</h2><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">&apos;代码&apos;</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">代码块</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure><p><code>print(&quot;hello world!&quot;)</code><br><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">print(&quot;hello world!&quot;)</span><br><span class="line">print(&quot;hello world!!!&quot;)</span><br></pre></td></tr></table></figure></p><h2 id="7-列表"><a href="#7-列表" class="headerlink" title="7. 列表"></a>7. 列表</h2><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">- [x] 支持以 PDF 格式导出文稿</span><br><span class="line">- [x] 改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率</span><br><span class="line">- [ ] 新增 Todo 列表功能</span><br><span class="line">- [ ] 修复 LaTex 公式渲染问题</span><br><span class="line">- [ ] 新增 LaTex 公式编号功能</span><br></pre></td></tr></table></figure><ul><li style="list-style: none"><input type="checkbox" checked> 支持以 PDF 格式导出文稿</li><li style="list-style: none"><input type="checkbox" checked> 改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率</li><li style="list-style: none"><input type="checkbox"> 新增 Todo 列表功能</li><li style="list-style: none"><input type="checkbox"> 修复 LaTex 公式渲染问题</li><li style="list-style: none"><input type="checkbox"> 新增 LaTex 公式编号功能</li></ul><h2 id="8-表格"><a href="#8-表格" class="headerlink" title="8. 表格"></a>8. 表格</h2><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">| 项目| 价格| 数量|</span><br><span class="line">| -| :-:| -:|</span><br><span class="line">| 计算机| $1600| 5|</span><br><span class="line">| 手机| $12| 12|</span><br><span class="line">| 管线| $1| 234|</span><br></pre></td></tr></table></figure><table><thead><tr><th>项目</th><th style="text-align:center">价格</th><th style="text-align:right">数量</th></tr></thead><tbody><tr><td>计算机</td><td style="text-align:center">$1600</td><td style="text-align:right">5</td></tr><tr><td>手机</td><td style="text-align:center">$12</td><td style="text-align:right">12</td></tr><tr><td>管线</td><td style="text-align:center">$1</td><td style="text-align:right">234</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-标题&quot;&gt;&lt;a href=&quot;#1-标题&quot; class=&quot;headerlink&quot; title=&quot;1. 标题&quot;&gt;&lt;/a&gt;1. 标题&lt;/h2&gt;&lt;figure class=&quot;highlight plain hljs&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;# 一级标题&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;## 二级标题&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://garyliu0816.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="markdown" scheme="https://garyliu0816.github.io/tags/markdown/"/>
    
  </entry>
  
</feed>
